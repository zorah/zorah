---
layout: archive
title: "Open Student Projects"
permalink: /open_projects/
author_profile: true
---

{% include base_path %}

## How To Apply

Please read this section carefully. I might not answer you if you do not follow the instructions and I am very busy at the moment (or just reply with a link here).

In order to apply write an email to laehner [at] uni-bonn.de with at least the following information:
- a short introduction of yourself including your study program, semester and programming languages you have experience with
- which of the projects below you want to do, or a short proposal of your own topic (make sure to point out how the topic is related to my research)
- your transcript of records (this is the list of your courses and grades you can get from Unisono)

Optional but appreciated:
- your CV
- a description of experience you have with handling geometric data (for example the lecture "Advanced Topics in Computer Graphics II") 
- any questions/constraints you have for your thesis


## Available Projects

Currently available spots: 2 

All projects related to deep learning need to be implemented in PyTorch.

### Analysis of Autoencoder Latent Spaces

*Summary:* Survey, implement and compare existing autoencoder architectures for point clouds, voxel grids and triangle meshes.

*Suitable for:* Bachelor Thesis, Master Thesis

*Requirements:* Python, passed our Deep Learning lecture (exception for Bachelor students)


### Analysis of Functional Maps framework using different basis sets

*Summary:* Survey, implement and compare the behavior of functional maps when replacing the default Laplace-Beltrami eigenbasis with different sets of basis functions.

*Suitable for:* Bachelor Thesis, Master Thesis

*Requirements:* Matlab or Python experience

*Related Reading:* [Functional Maps: A Flexible Representation of Maps Between Shapes, Ovsjanikov et al., 2012](https://www.lix.polytechnique.fr/~maks/papers/obsbg_fmaps.pdf) 
[Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching, Bastian et al., 2023](https://arxiv.org/abs/2312.03678)

